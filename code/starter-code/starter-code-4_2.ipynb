{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Lab\n",
    "\n",
    "In this lab we will further explore Scikit's and NLTK's capabilities to process text. We will use the 20 Newsgroup dataset, which is provided by Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.datasets.base.Bunch'>\n",
      "2034\n",
      "1353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(data_train)\n",
    "print len(data_train.data)\n",
    "print len(data_test.data)\n",
    "data_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize CountVectorizer, fit and get features\n",
    "v = CountVectorizer()\n",
    "v.fit(data_train.data)\n",
    "len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26576"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With English Stop words\n",
    "v = CountVectorizer(stop_words='english')\n",
    "v.fit(data_train.data)\n",
    "len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train = v.transform(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'data': 0,\n",
       " u'does': 1,\n",
       " u'don': 2,\n",
       " u'edu': 3,\n",
       " u'god': 4,\n",
       " u'good': 5,\n",
       " u'graphics': 6,\n",
       " u'image': 7,\n",
       " u'jesus': 8,\n",
       " u'just': 9,\n",
       " u'know': 10,\n",
       " u'like': 11,\n",
       " u'nasa': 12,\n",
       " u'people': 13,\n",
       " u'say': 14,\n",
       " u'space': 15,\n",
       " u'think': 16,\n",
       " u'time': 17,\n",
       " u'use': 18,\n",
       " u'way': 19}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 words\n",
    "top_20 = CountVectorizer(stop_words='english', max_features = 20)\n",
    "top_20.fit_transform(data_train.data)\n",
    "top_20.get_feature_names()\n",
    "top_20.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi,\\n\\nI've noticed that if you only save a mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\nSeems to be, barring evidence to the contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n &gt;In article &lt;1993Apr19.020359.26996@sq.sq.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I have a request for those who would like to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>AW&amp;ST  had a brief blurb on a Manned Lunar Exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               data\n",
       "0      1  Hi,\\n\\nI've noticed that if you only save a mo...\n",
       "1      3  \\n\\nSeems to be, barring evidence to the contr...\n",
       "2      2  \\n >In article <1993Apr19.020359.26996@sq.sq.c...\n",
       "3      0  I have a request for those who would like to s...\n",
       "4      2  AW&ST  had a brief blurb on a Manned Lunar Exp..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['class'] = data_train.target\n",
    "df['data'] = data_train.data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "[u'argument', u'atheism', u'atheists', u'believe', u'bible', u'does', u'don', u'god', u'jesus', u'just', u'know', u'like', u'people', u'religion', u'said', u'say', u'think', u'time', u'true', u'way']\n",
      "\n",
      "1 \n",
      "[u'available', u'color', u'data', u'does', u'edu', u'file', u'files', u'format', u'ftp', u'gif', u'graphics', u'image', u'images', u'jpeg', u'know', u'like', u'program', u'pub', u'software', u'use']\n",
      "\n",
      "2 \n",
      "[u'data', u'don', u'earth', u'just', u'launch', u'like', u'lunar', u'mission', u'moon', u'nasa', u'new', u'orbit', u'people', u'program', u'satellite', u'shuttle', u'space', u'time', u'use', u'year']\n",
      "\n",
      "3 \n",
      "[u'believe', u'bible', u'christian', u'did', u'does', u'don', u'god', u'good', u'jesus', u'just', u'know', u'life', u'like', u'people', u'point', u'said', u'say', u'think', u'time', u'way']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 20 most common words in each category \n",
    "for i in range(0,4):\n",
    "    top_20.fit_transform(df[df['class'] == i]['data'])\n",
    "    print i,'\\n', top_20.get_feature_names()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy Score: 0.745011086475\n",
      "Confusion Matrix: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "Number of Features: 26576\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for features from a vectorizer. \n",
    "# Pass a vectorizer to bellow function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def log_reg(vect):\n",
    "    X_train = vect.fit_transform(data_train.data)\n",
    "    y_train = data_train.target\n",
    "    X_test = vect.transform(data_test.data)\n",
    "    y_test = data_test.target\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    print 'Accuracy Score:', lr.score(X_test, y_test)\n",
    "    print 'Confusion Matrix: \\n', confusion_matrix(y_test,y_pred)\n",
    "\n",
    "v = CountVectorizer(stop_words='english')\n",
    "log_reg(v)\n",
    "print 'Number of Features:', len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.49519586105\n",
      "Confusion Matrix: \n",
      "[[159  98  30  32]\n",
      " [ 30 296  55   8]\n",
      " [ 47 147 186  14]\n",
      " [112  91  19  29]]\n",
      "Number of Features: 20\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words='english',max_features=20)\n",
    "log_reg(v)\n",
    "print 'Number of Features:', len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.747967479675\n",
      "Confusion Matrix: \n",
      "[[187  16  47  69]\n",
      " [  8 347  30   4]\n",
      " [ 20  27 335  12]\n",
      " [ 67  17  24 143]]\n",
      "Number of Features: 12144\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words='english', max_df=0.2, min_df= 2)\n",
    "log_reg(v)\n",
    "print 'Number of Features:', len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.589061345159\n",
      "Confusion Matrix: \n",
      "[[160  67  33  59]\n",
      " [ 25 314  44   6]\n",
      " [ 37  88 247  22]\n",
      " [ 90  70  15  76]]\n",
      "Number of Features: 54\n"
     ]
    }
   ],
   "source": [
    "# top 20 words for each category,  set will remove duplicates\n",
    "words = []\n",
    "top_20 = CountVectorizer(stop_words='english', max_features = 20)\n",
    "for i in range(0,4):\n",
    "    top_20.fit_transform(df[df['class'] == i]['data'])\n",
    "    words += (top_20.get_feature_names())\n",
    "    \n",
    "v = CountVectorizer(stop_words='english',vocabulary=set(words))\n",
    "log_reg(v)\n",
    "print 'Number of Features:', len(v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashing and TF-IDF\n",
    "\n",
    "Let's see if Hashing or TF-IDF improves the accuracy.\n",
    "\n",
    "1. Initialize a HashingVectorizer and repeat the test with no restriction on the number of features\n",
    "- does the score improve with respect to the count vectorizer?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model\n",
    "- Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "- can you improve on your best score above?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.736881005174\n",
      "Confusion Matrix: \n",
      "[[197  15  65  42]\n",
      " [  9 347  32   1]\n",
      " [ 21  23 350   0]\n",
      " [ 86  18  44 103]]\n",
      "No. of features: 1048576\n"
     ]
    }
   ],
   "source": [
    "h = HashingVectorizer(stop_words='english')\n",
    "log_reg(h)\n",
    "print 'No. of features:', h.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.730229120473\n",
      "Confusion Matrix: \n",
      "[[187  18  64  50]\n",
      " [  8 347  33   1]\n",
      " [ 19  22 352   1]\n",
      " [ 85  19  45 102]]\n",
      "No. of features: 50000\n"
     ]
    }
   ],
   "source": [
    "h = HashingVectorizer(stop_words='english', n_features=50000)\n",
    "log_reg(h)\n",
    "print 'No. of features:', h.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.733185513673\n",
      "Confusion Matrix: \n",
      "[[199  26  55  39]\n",
      " [ 11 349  27   2]\n",
      " [ 17  39 338   0]\n",
      " [ 86  25  34 106]]\n",
      "No. of features: 26879\n"
     ]
    }
   ],
   "source": [
    "# TFIDF \n",
    "T = TfidfVectorizer()\n",
    "log_reg(T)\n",
    "print 'No. of features:', len(T.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.749445676275\n",
      "Confusion Matrix: \n",
      "[[196  16  58  49]\n",
      " [  7 351  30   1]\n",
      " [ 19  21 354   0]\n",
      " [ 78  16  44 113]]\n",
      "No. of features: 11190\n"
     ]
    }
   ],
   "source": [
    "# TFIDF \n",
    "T = TfidfVectorizer(stop_words='english', min_df=2, token_pattern='[a-zA-Z]{3,50}')\n",
    "log_reg(T)\n",
    "print 'No. of features:', len(T.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifier comparison\n",
    "\n",
    "Of all the vectorizers tested above, choose one that has a reasonable performance with a manageable number of features and compare the performance of these models:\n",
    "\n",
    "- KNN\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "In order to speed up the calculation it's better to vectorize the data only once and then compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "score: 0.288987435329\n",
      "confusion matrix: \n",
      "[[108 100  57  54]\n",
      " [132 140  43  74]\n",
      " [113 124  92  65]\n",
      " [ 77  80  43  51]]\n",
      "No. of features: 11190\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "score: 0.749445676275\n",
      "confusion matrix: \n",
      "[[196  16  58  49]\n",
      " [  7 351  30   1]\n",
      " [ 19  21 354   0]\n",
      " [ 78  16  44 113]]\n",
      "No. of features: 11190\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "score: 0.591278640059\n",
      "confusion matrix: \n",
      "[[144  52  48  75]\n",
      " [ 21 309  47  12]\n",
      " [ 43  73 256  22]\n",
      " [ 77  55  28  91]]\n",
      "No. of features: 11190\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "score: 0.291204730229\n",
      "confusion matrix: \n",
      "[[  0   0 319   0]\n",
      " [  0   0 389   0]\n",
      " [  0   0 394   0]\n",
      " [  0   0 251   0]]\n",
      "No. of features: 11190\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "score: 0.654841093865\n",
      "confusion matrix: \n",
      "[[169  23  42  85]\n",
      " [ 17 336  26  10]\n",
      " [ 35  49 272  38]\n",
      " [ 92  22  28 109]]\n",
      "No. of features: 11190\n",
      "\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "score: 0.685883222469\n",
      "confusion matrix: \n",
      "[[197  36  32  54]\n",
      " [ 19 337  30   3]\n",
      " [ 36  61 276  21]\n",
      " [ 81  33  19 118]]\n",
      "No. of features: 11190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = [KNeighborsClassifier(), LogisticRegression(), DecisionTreeClassifier(), \n",
    "                SVC(), RandomForestClassifier(), ExtraTreesClassifier()]\n",
    "\n",
    "def model_score(model, v):\n",
    "    X_train = v.fit_transform(data_train.data)\n",
    "    X_test = v.transform(data_test.data)\n",
    "    y_train = data_train.target\n",
    "    y_test = data_test.target\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_predictions = model.predict(X_test)\n",
    "    \n",
    "    print 'score:', model.score(X_test, y_test)\n",
    "    print 'confusion matrix:','\\n', confusion_matrix(y_test, y_predictions)\n",
    "    print 'No. of features:', len(T.get_feature_names())\n",
    "    print \n",
    "\n",
    "T = TfidfVectorizer(stop_words='english', min_df=2, token_pattern='[a-zA-Z]{3,50}')\n",
    "\n",
    "for i in models:\n",
    "    print i\n",
    "    model_score(i,T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other classifiers\n",
    "\n",
    "Adapt the code from [this example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py) to compare across all the classifiers suggested and to display the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: NLTK\n",
    "\n",
    "NLTK is a vast library. Can you find some interesting bits to share with classmates?\n",
    "Start here: http://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
